v1.1 141124
CAUSALFound - Methods in detail
________________


Section 1: Synthetic Data Generation and Target Trial Emulation


Generation of EHR feature and conditioning feature value ranges
The ‘origin’ dataset for development of this foundation model will be the SCI-Diabetes data (diabetes domain specific data) and linked data from one (or more) Scottish Health Boards. Linked data includes - all laboratory investigation results, prescribing information (prescription and encashment data), inpatient and outpatient coding data and date / cause of death data.
We will generate EHR feature inputs systematically from permutations of plausible values across all features of interest. Features will be binned using a quantile-based approach that divides the available data feature according to the density distribution.
Conditioning features will be generated similarly, eg for therapeutic agents all plausible combinations of therapies (constrained by instances in source data and guidelines) will be generated.


For each (EHR feature input example) and (conditioning feature) pairing, synthetic cohorts (with controlled deviation per feature) will be generated (described below).


Use of multiple generative models for synthetic data generation
We will implement a multi-method approach to synthetic data generation, leveraging complementary strengths of different generative architectures.


In the first instance, 2 generative approaches will be considered for implementation, drawn from the established and novel methods (examples listed below). Code will be constructed with a modular architecture, in order that methods may be added / removed as required without significant rewriting:


* Method 1 - A partial conditional VAE that accepts conditioning information for a subset of features, enabling more targeted generation while maintaining flexibility in non-conditioned features. This architecture includes input channels for conditioning variables and employs a modified loss function to balance reconstruction quality with adherence to conditioning constraints.
* Method 2 - A conditional GAN architecture (CT-GAN) specifically designed for tabular data generation, utilising a discriminator trained to assess both the realism of generated sequences and their adherence to conditioning variables.
* Method 3 - A time-series GAN implementing a recurrent architecture in both generator and discriminator networks, with specific attention mechanisms to capture long-term dependencies in the temporal data.
* Method 4 - A diffusion-based model adapted for temporal data generation, implementing a gradual denoising process that preserved temporal consistency and feature relationships.


Each implemented model will be trained independently on the same underlying dataset, with architecture-specific optimizations and hyperparameter tuning to maximise performance.


Synthetic Cohort Generation
The multi-method approach will generate separate synthetic cohorts, maintaining clear provenance tracking for all synthetic patients. For each target phenotype, we will generate multiple synthetic cohorts using each generative method:


* Via the partial conditional VAE, using specific feature conditioning to guide generation while allowing variation in other characteristics.
* Using the conditional GAN to generate cohorts with precise conditioning on target phenotype characteristics.
* Through the time-series GAN, focusing on temporal consistency and pattern preservation.
* Via the diffusion model, using guided denoising to generate phenotype-aligned synthetic patients.


Each synthetic cohort will undergo independent quality control processes, including:
* Validation against real data distributions
* Assessment of clinical plausibility
* Verification of temporal consistency
* Evaluation of feature correlation preservation
* Checking adherence to physiological constraints


Synthetic cohorts will be maintained separately based on their generative source, enabling traceability and method-specific quality assessment. This separation allows for comparative analysis of synthetic data quality and subsequent evaluation of the influence different synthetic sources might have on trial emulation results.


Target Trial Emulation Process
Using the multiple synthetic cohorts, we will conduct systematic trial emulations while maintaining separation between cohorts generated using different methods. The trial emulation framework incorporates:


1.        A clinically plausible set of interventions - defined by clinical guideline, including individual therapies, and combinations of therapies
2.        Temporal dynamics of treatment effects
3.        Multiple outcome measures across various timepoints
4.        A design that aims to minimise potential confounding factors


Trial Emulation will adhere to the principles outlined by Hernan et al, to minimise the impact of confounders and predict effect size. Effect sizes will be calculated using appropriate statistical methods for each outcome type (e.g., hazard ratios for time-to-event data, mean/median differences for continuous outcomes).


Multiple permutations of each trial will be conducted using different synthetic cohorts, maintaining separation between generative methods to assess both the variability in treatment effects and the impact of different synthetic data sources. The emulation process may account for:
-        Treatment-specific effects
-        Time-varying confounding
-        Effect modification by patient characteristics
-        Temporal patterns of response
-        Safety outcomes and adverse events


The output of each trial emulation will include:
-        Primary effect size estimates with confidence intervals
-        Time-course of treatment effects
-        Uncertainty quantification for all estimates
-        Source-specific analyses comparing results across different synthetic data generation methods


This comprehensive approach, utilising multiple generative methods and maintaining separate cohorts, will provide a robust framework for assessing treatment effects while enabling analysis of the influence of different synthetic data generation approaches on emulation results.


________________


Section 2: Calibration and Manifold Generation Methods
(i) Initial Calibration Against RCT Data


We will develop a systematic calibration process using available randomised controlled trial (RCT) data to adjust trial emulation outputs. For each available RCT with matching inclusion/exclusion and conditioning criteria, we will identify corresponding synthetic cohorts and trial emulations. The calibration process will involve:


1.        Mapping RCT populations to the feature space of our synthetic cohorts using standardised phenotypic characteristics
2.        Comparing predicted effect sizes from trial emulations with observed effect sizes from RCTs
3.        Computing calibration coefficients that align predicted effects with observed outcomes across multiple dimensions:
* Primary outcome measures
* Time-course of effects
* Subgroup-specific responses (if relevant)


(ii) Generation of the Calibration Manifold
The calibration process will identify systematic relationships between population characteristics and calibration needs, leading to the development of a calibration manifold. This manifold will represent a continuous, multi-dimensional surface in the space of population characteristics and effect size adjustments. The manifold structure captures how calibration requirements vary across different population phenotypes and treatment contexts.


Manifold construction will involve the following stages:
1.        Embedding of population characteristics and calibration coefficients into a shared high-dimensional space using:
* Phenotypic features
* Treatment characteristics
* Outcome types
* Temporal aspects
* Calibration coefficients
2.        Dimensionality reduction while preserving local geometric structure:
* Application of manifold learning techniques
* Preservation of neighbourhood relationships
* Maintenance of continuous paths between similar populations
* Conservation of uncertainty information
3.        Characterization of manifold properties:
* Local curvature analysis
* Density estimation in different regions
* Identification of principal directions of variation
* Mapping of uncertainty landscapes


We will investigate several key properties of the calibration manifold:
-        Smoothness: The extent to which calibration varies continuously across similar populations
-        Local structure: Whether nearby points in phenotype space require similar calibrations
-        Geometric interpretability: How distances on the manifold correspond to differences in calibration requirements
-        Natural uncertainty quantification: Based on local geometric properties and data density


(iii) Extension to Uncalibrated Populations
For populations without available RCT data for direct calibration, we will develop a geometric approach using the calibration manifold structure. This method will enable estimation of calibration coefficients with appropriate uncertainty scaling based on manifold properties.


The process for uncalibrated populations will involve:
1. Population Mapping:
   -        Embedding of the new population's characteristics in the manifold space
   -        Identification of nearest calibrated populations
   -        Analysis of local manifold properties
2. Calibration Estimation:
   -        Computation of calibration coefficients using local manifold structure
   -        Weighted averaging of nearby calibration coefficients
   -        Geometric interpolation accounting for manifold curvature
3. Uncertainty Quantification:
   -         Scaling of uncertainty estimates based on:
     *        Distance from known calibration points
     *        Local manifold curvature
     *        Density of nearby calibration points
     *        Consistency of local calibration values
     * Population similarity metrics


This uncertainty estimation process will ensure that confidence in calibration increases with:
-        Proximity to well-calibrated populations
-        Higher density of nearby calibration points
-        Lower local manifold curvature
-        Greater consistency in local calibration values
-        More typical phenotype characteristics
4. Validation:
   -        Leave-one-out cross-validation on known RCT populations
   -        Assessment of uncertainty calibration
   -        Sensitivity analysis to manifold parameters
   -        Evaluation of edge cases and boundary conditions


This approach should provide a principled method for extending calibration to populations without direct RCT validation while maintaining appropriate uncertainty quantification. The method will be particularly valuable for:
-        Rare disease populations
-        Understudied demographic groups
-        Novel combination therapies
-        Edge case phenotypes
-        Resource-limited settings where RCTs are impractical


In summary, the calibration manifold framework will enable:
-        Systematic adjustment of trial emulation results
-        Principled extension to new populations
-        Appropriate uncertainty scaling
-        Identification of populations needing additional validation
-        Guidance for prioritizing future RCTs


These calibration methods provide a robust framework for improving the accuracy of trial emulation results while maintaining appropriate uncertainty quantification, particularly for populations where direct RCT validation is unavailable.


________________


Section 3: Foundation Model Development
Conceptual Framework
This foundation model will be designed to learn generalizable representations of relationships between patient characteristics, interventions, and treatment effects from the previously described large-scale trial emulation data. The key intuition is that by jointly embedding patient features, intervention specifications, and effect sizes into a shared latent space, the model can capture complex patterns of treatment response and enable transfer learning across different clinical scenarios.


The model architecture will be structured to reflect the inherent relationships between three key components:
1        Individual potential patient phenotypes and trajectories from electronic health records (EHR)
2.        Therapeutic intervention specifications including drugs, doses, and temporal patterns
3.        Effect sizes with associated uncertainties from trial emulations


This tripartite structure will allow the model to learn not only direct associations between interventions and outcomes but also how these relationships vary across different patient phenotypes and contexts.


Model Architecture
The foundation model will implement a multi-stream architecture with specialised processing pathways for different types of input data:
1. EHR Feature Processing Stream:
   -        Static feature embedding layers for demographic and baseline characteristics
   -        Temporal feature processing using attention-based architectures for longitudinal data
   -        Feature interaction layers to capture complex relationships between clinical variables
2. Intervention Processing Stream:
   -        Dedicated embedding layers for intervention specifications
   -        Temporal intervention aspect processing
   -        Multi-intervention interaction modelling
   3. Effect Size Processing Stream:
   -        Specialised embedding layers for different outcome types
   -        Uncertainty-aware processing incorporating confidence intervals
   -        Temporal effect profile encoding


The envisaged core embedding architecture will consist of:
1. Fusion Layer:
   -        Cross-attention mechanisms to integrate information across streams
   -        Learnable attention weights for dynamic feature importance
   -        Residual connections to preserve stream-specific information
2. Transformer Blocks:
   -        Multi-head attention mechanisms for complex pattern recognition
   -        Feed-forward networks with layer normalisation
   -        Position encoding for temporal aspects
3. Embedding Projection:
   -        Dimensionality reduction to create efficient representations
   -        Normalisation layers to stabilise training
   -        Output space structuring to facilitate downstream tasks


Multiple task-specific heads will be implemented for training:
-        Effect size reconstruction
-        Intervention-effect prediction
-        Phenotype clustering
-        Temporal trajectory prediction


Training Strategy
The training process will be implemented in multiple phases to ensure robust learning of generalizable representations:


1. Pre-training Phase:
   -        Self-supervised learning using masked prediction tasks:
     *        Random masking of effect sizes
     *        Prediction of intervention effects
     *        Recovery of masked feature values
   -        Contrastive learning between similar cases to build robust embeddings
   -        Multi-task learning across different prediction objectives
2. Training Objectives:
   The total loss function combines multiple components:


   Total_Loss = α * Effect_Size_Loss 
               + β * Feature_Reconstruction_Loss
               + γ * Contrastive_Loss
               + δ * Uncertainty_Loss


   where:
   - Effect_Size_Loss captures accuracy in treatment effect prediction
   - Feature_Reconstruction_Loss ensures meaningful feature representations
   - Contrastive_Loss improves embedding space structure
   - Uncertainty_Loss calibrates confidence estimates


   Loss weights (α, β, γ, δ) were dynamically adjusted during training using a curriculum learning strategy.


3. Training Process Implementation:
   -        Progressive layer unfreezing to stabilise training
   -        Dynamic batch composition to ensure diverse learning examples
   -        Gradient accumulation for effective batch size management
   -        Mixed precision training for computational efficiency


4. Validation Strategy:
   -        Hold-out validation set for performance monitoring
   -        Cross-validation for stability assessment
   -        Out-of-distribution testing to evaluate generalisation
   -        Embedding space quality metrics evaluation
   -        Uncertainty calibration assessment


The training process will incorporate several technical optimizations:
-        Gradient checkpointing for memory efficiency
-        Mixed precision training
-        Distributed training across multiple GPUs
-        Memory-efficient attention mechanisms


Monitoring and maintenance protocols will include:
-        Continuous tracking of training metrics
-        Regular evaluation of embedding space quality
-         Detection of potential data drift
-        Assessment of prediction calibration
-        Validation of uncertainty estimates


The resulting foundation model will provide a robust framework for:
-        Learning complex treatment effect patterns
-        Generating meaningful embeddings of clinical scenarios
-        Capturing uncertainty in predictions
-        Enabling transfer learning to new clinical contexts
-        Supporting downstream task-specific fine-tuning


This architecture and training approach will create a versatile foundation model capable of capturing complex relationships between patient characteristics, interventions, and treatment effects, while maintaining appropriate uncertainty quantification and enabling efficient adaptation to specific clinical scenarios through fine-tuning.