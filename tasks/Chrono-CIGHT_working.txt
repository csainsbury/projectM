Chrono-CIGHT: A Time-Aware Multimodal Foundation Model for Longitudinal Retinal Image Analysis in People with Diabetes

Background. Type 2 diabetes is a chronic condition that develops and progresses over many years. People with diabetes are at risk of developing a variety of complications, characterised as micro- and macro-vascular in origin, which may have a significant impact on survival and quality of life. Regular screening programmes are in place, aiming to identify and intervene if complications - or early signs of elevated risk of complications - are present, allowing for appropriate intervention.  In Scotland, screening consists of regular laboratory  investigations (including HbA1c), physiological measurements (weight and blood pressure), examination of the feet, and retinal screening. The majority of these investigations are performed yearly, with retinal screening performed at intervals between 6 to 24 months (depending on assessed level of risk).  The data accruing from these screening programmes - along with additional information associated with interim investigations within both primary and secondary care are contained within SCI-Diabetes, the national diabetes database for Scotland. This data can be linked (via CHI number) to datasets holding information on hospital admissions, outpatient appointments, prescribed therapeutics, date/cause of death data and the retinal images described above.
Within the Scottish NHS, we are able access a large dataset of retinal images spanning many years (2006 onwards), which can be linked to domain-specific information within SCI Diabetes, in addition to additional relevant datasets such as laboratory results within SCI Store, prescribing information, coded inpatient and outpatient diagnoses and cause of death information. Our current dataset consists of approximately c.900,000 images, and we aim to add a further <large number> of images from NHS Greater Glasgow and Clyde in the course of this project.

Given the pathophysiology of type 2 diabetes, retinal features change over time as the retina experiences the impact of hyperglycemia and other factors. These progressive changes are likely to be informative when predicting future outcomes.

RetFound[1] - a retinal Foundation Model published in 2023, has demonstrated utility in providing a basis for the rapid development of high-quality predictive models for clinically important cardiovascular and non-cardiovascular outcomes. RetFound was trained on a large dataset that included a high proportion of retinal images from individuals with diabetes. The final (image-derived) RetFound foundation model was trained on images from a single time point, which may limit its ability to capture relevant and important information given the progressive nature of type 2 diabetes and its complications over relatively prolonged time scales.

Aims. We aim to develop a time-aware foundation model using multimodal data, which is routinely collected from people with diabetes. The model will incorporate both time-sequence and time-invariant data from linked EHR datasets (including demographic data, disease related laboratory and physiological measurements, longitudinal retinal images, healthcare engagement information and clinical comments).
By developing such a time-aware foundation model, we aim to improve the prediction of diabetes-related complications and ultimately contribute to better patient outcomes. Specifically we aim to identify individuals who - using available risk stratification tools - would currently be categorised as low risk, but who are identified as high risk using our tool. This would potentially identify a group of individuals in whom treatment might be started at an earlier time point, preventing the development of diabetes-associated complications (pending appropriate clinical validation/prospective trials).
Separate models will be constructed for both type 1 and type 2 diabetes, with variants of the model being trained for retinal image data, EHR data and combined image and EHR datasets.

Primary objectives of the project:
i.	To develop a time-aware foundation model based on a transformer architecture using the combined Tayside & Fife, and Greater Glasgow and Clyde retinal image dataset. This model will take as input sequences of retinal images along with date and timestamp information, and linked EHR data allowing the model to capture information relating to temporal changes in these features.
ii.	To demonstrate the utility of this model by fine tuning the trained foundation model(s) to predict outcomes of clinical importance in the domain of diabetes from sequential data drawn from NHS Tayside & Fife and NHS Greater Glasgow & Clyde, with external validation being performed within a 3rd NHS Scotland Health Board - NHS Grampian. Outcomes of interest will include glycaemic control, weight change,  development of microvascular complications (incident microalbuminuria, increase in CKD category (decrease in renal function), incident retinopathy at various stages and progression of diabetes foot risk score (indicating neuropathy) and macro vascular complications (incident dementia, cardiovascular death, 3-, 4-, and 5-point MACE), and all-cause mortality. The ability of models created by fine-tuning the foundation model to predict non-cardiovascular endpoints will also be explored (eg Parkinson’s disease etc ).
iii.	To explore explainability of predictions within fine tuned models using SHAP (Shapley Additive Explanations) and other explainability methods to identify the features that are most important to the model when making decisions, and areas of images that contribute most to the outcome.
iv.	to develop and pilot technical infrastructure for implementing candidate models within SCI Diabetes, used to trial cluster-randomisation of AI for quality improvement in realworld health care delivery..   
v.	To assess the perceptions and acceptability of people with diabetes and other key stakeholders for using AI technology as part of the healthcare decision making process for diabetes care in Scotland.

Method outline

Summary:
1 create model in Tayside + GGC image datasets. ? import GGC images with linked data -> would probably require formal ethics
2 fine-tune in Tayside (we have linked data)
3 out-of-sample validate in Grampian

Retinal Data
- Data curation (retinal images): We will combine the NHS Tayside and NHS GGC retinal image datasets into a single large-scale dataset of longitudinal retinal images, with multiple images per patient over several years. This data will be held within the HIC TRE, and will be comprised of colour photography (CFP) images. The dataset will by default cover a wide range of individual demographics, imaging devices, and disease stages and comorbidities, as these images are drawn from the total population of people with diabetes attending retinal screening.
- Self-supervised pretraining: We will develop a self-supervised pretraining approach that learns both spatial and temporal representations from the longitudinal retinal image dataset. We will adapt a masked autoencoding approach similar to that used in RETFound to reconstruct masked spatial regions and entire masked temporal slices. This will encourage the model to learn the temporal structure and progression of retinal findings. Additionally, we will introduce a temporal ordering pretext task, where the model is trained to sort shuffled images from a patient into the correct chronological order, with the aim of assisting in learning disease progression patterns.
- Temporal transformer architecture: We will design a temporal transformer architecture that extends the vision transformer (similar to that used in RETFound) to process sequences of retinal images. We will incorporate positional encodings to represent the relative time between images in the sequence. Self-attention layers in the transformer will attend to both spatial locations within each image and interactions between images across time. This architecture will enable the model to capture the complex spatial and temporal dependencies in the longitudinal retinal image data.
- Fine-tuning for downstream tasks: We will fine-tune the pretrained time-aware foundation model for specific downstream tasks, including prediction of micro- and macro-vascular complications as outlined above (?in training or a separate test dataset). The model will subsequently be adaptable to different tasks via the process of adding task-specific heads on top of the pretrained temporal transformer architecture.[2]
- Evaluation of spatial and temporal performance: We will evaluate the model's performance on both spatial and temporal aspects of retinal image analysis, and assess the its ability to detect findings in single images (spatial performance) in addition to its performance in forecasting future condition onset and progression based on the temporal patterns in the longitudinal data (temporal performance). We will perform internal validation on held-out test sets (within the training data from GGC/Tayside) and external validation on out of sample data (Grampian) to investigate the model's performance. [3]
- A comparison to existing models: We will compare the performance of the time-aware foundation model to existing state-of-the-art models, such as RETFound and ?FLAIR, evaluating the models on the same downstream tasks and publicly available (where possible) datasets to provide a true comparison. This comparison will give a measure of the benefit of incorporating temporal information - by comparing the time-aware model's performance to that of models trained on static images alone.
- (Potentially, assess the model's ability to generalise to new patient populations, imaging devices, and conditions / complications not seen during training.)[4]

EHR Data
- Data curation (EHR data): relevant data from individuals from the NHS Tayside and Fife SCI Diabetes datasets containing diagnostic, screening, physiological and demographic data (nT1 = , nT2 = ) will be linked to laboratory investigation results, coded inpatient and outpatient admissions / investigations information, prescribing data and death certification information. Clinically relevant time series features will be extracted alongside time-independent data for each individual.
- 

Potential Utility
- A time-aware foundation model for retinal image analysis has the potential to improve the accuracy of complication and incident disease predictions by taking advantage of the temporal information inherent in longitudinal retinal imaging data. In the process of learning the patterns of disease progression over time, the model may identify subtle features that may not be apparent in single, static images. This could lead to more performant models with greater potential clinical impact.
- Modelling the temporal progression patterns of retinal findings may provide insight into the natural history and risk factors for both ocular and systemic diseases. Ultimately this has the potential to inform the development of new screening and prevention strategies, as well as personalised treatment approaches tailored to individual’s risk profiles.
- By learning the subtle temporal patterns that precede clinically significant changes, the model may have the potential to alert clinicians to people at high risk of developing adverse outcomes, allowing earlier intervention, potentially preventing irreversible vision loss or systemic complications, and assisting in clinical decision making and optimal healthcare resource usage.
- A comparison of the performance of the time-aware foundation model to existing models - such as RETFound - will demonstrate the value (positive or negative) of incorporating temporal information in retinal image analysis. Evaluating the performance of models on the same downstream tasks and datasets, will allow quantification of the improvement in prediction accuracy and early detection of disease onset achieved by the time-aware model. This comparison may also reveal specific temporal patterns and risk factors that static models may overlook.
- A successful time-aware retinal image foundation model could serve as a valuable resource for future research and clinical applications in ophthalmology and within the diabetes domain in general. A method to allow learning of robust and generalisable temporal features has the potential to be applied to other medical imaging domains where longitudinal data is available, such as neuroimaging or cancer screening. Insight gained from analysis of temporal progression of retinal findings could inform the development of new imaging biomarkers and predictive models for the complications and diseases investigated. The time-aware foundation model could also be used to generate large-scale, labelled datasets for training future machine learning models (cf cheXpert etc in CXR imaging).



Work Packages
WP1
- Permissions / Ethics and  Data provisioning.
- NHS Tayside/Fife: images + data
- NHS GGC: images +/- data
- NHS Grampian: images + data
WP2.
- Construction of self-supervised EHR and image transformer-based foundation models
- Fine tuning of foundation model(s) to specific tasks with internal validation
WP3.
- External validation
- Explainability
WP4.
- SCI-Diabetes deployment architecture for pilot evaluation
- ?which board GGC or Grampian[5]
WP5.
- PPI - Involvement at all stages of the project with representatives on regular steering group meetings
WP6.
- economic impact modelling
WP7.
- Writing and Dissemination
- Protocol, Methodological output, implementation output



Outstanding Actions 
- Complete draft - CS to refine method and incorporate EHR data into the methodology
- Complete Caldicott application for NHS GGC images - send to Charlie Mayor (GGC Safe Haven manager)
- Plan workshop of potential collaborators
- Consider grant application - costings etc
- Transfer images from NHS GGC into the HIC environment
- PPI involvement / workshop


LLM incorporation
- narrative summary
- text summary trained bootstrapped
Silva-Rodriguez, Julio, Hadi Chakor, Riadh Kobbi, Jose Dolz, and Ismail Ben Ayed. 2023. “A Foundation LAnguage-Image Model of the Retina (FLAIR): Encoding Expert Knowledge in Text Supervision.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/2308.07898.

Zhou, Yukun, Mark A. Chia, Siegfried K. Wagner, Murat S. Ayhan, Dominic J. Williamson, Robbert R. Struyven, Timing Liu, et al. 2023. “A Foundation Model for Generalizable Disease Detection from Retinal Images.” Nature 622 (7981): 156–63.
this could be done within the tayside and fife data as we already have the linked data associated with these images
Could we also report performance on IDRID, MESSIDOR and APTOS, like in RETFound?

https://www.nature.com/articles/s41586-023-06555-x/figures/2
probably not possible due to data availability
either would be ok - but grampian might be preferable as smaller and possibly more straightforward to evaluate?
