Diabetes Risk Prediction Metamodels: Overall Summary of Process

Background: This project (implemented in R and Python) represents a system that predicts the risk of various complications in individuals with type 2 diabetes. The system leverages existing literature-based models, combines their predictions using metamodels, and incorporates sensitivity analysis and recalibration techniques. The overarching goal is to create robust and accurate predictive tools that are integrated into a clinical decision support system . The system was initially trained using a large dataset from Scottish Health Boards and then externally validated using data from North West London.

Data Preparation and Preprocessing: This initial phase involves preparing the raw clinical data for model training and evaluation. This is achieved through a series of scripts. Each script is specifically designed to handle the data requirements of an individual base model. These scripts load raw data from CSV files, including demographics, laboratory results, and diagnosis records. The raw data is transformed and cleaned to create consistent features. This cleaning process includes handling missing data through imputation (using the median value), and calculating critical time-to-event variables (t0 and t1) based on a fixed end_train_date, which serves as the start point for the prediction window. The final data is then saved to a directory as input files for each base model. Key functions used in this step, such as date conversions, ethnicity mapping, feature loading, and value extraction before a target date, are located in the shared functions file.

Base Model Implementation and Execution The core of the prediction system is made up of several literature-derived base models, each designed to predict a specific complication of diabetes. These models are implemented as individual callable functions. The base models include functions for predicting amputation risk, myocardial infarction (separately for men and women), blindness risk, renal failure risk, and mortality. Each model function takes preprocessed data, patient identifiers, and outcome values as inputs, and returns a dataframe that includes the original input data, and a new column of risk predictions.

The main model script manages the execution of these base models, with a sensitivity analysis that can be performed on any feature through its range of possible values. It iterates through different prediction window years (1, 2, 5 and 10) and a specified list of features to adjust (if specified). When a feature adjustment is required, a sensitivity coefficient is applied to that feature within the model. All intermediate outputs of base model predictions with sensitivity adjusted features are saved. After running the models with adjusted features, the base models are run once more with no adjustments, to generate base predictions for the metamodel. All predictions, both base and sensitivity adjusted are saved to a common intermediates directory. The script then executes the metamodel on the base model outputs to generate combined risk predictions and also executes the hba1c specific metamodel. Finally, all base model and metamodel prediction are combined across the different time windows.

Metamodel Generation and Application: To enhance predictive performance, the system implements a metamodel approach. The metamodel() function combines the predictions of the base models using a weighted linear combination. This functionâ€™s purpose is to create the input for the metamodels, and execute these models. It iterates through the features to be adjusted, and the sensitivity coefficients (if specified). It then iterates through each model and adds adjusted predictions if they are applicable to that model, otherwise the base (unadjusted) prediction is used. The resulting dataframes become the inputs for the metamodel function. The script also executes a number of specific metamodels which produce a single combined risk score for a number of combinations of complications (grouped as microvascular complications, macrovascular complications and anycomplication). The outputs of the metamodels are also saved out to a specified directory.

Sensitivity Analysis and Visualization: To understand the impact of individual input features on the final model outputs, the system includes a sensitivity analysis as previously mentioned. This is performed by systematically varying certain features according to the sensCoefs variable in the main model script. These coefficients are then applied to the specified features. The results of the sensitivity analysis are visualised, loading the generated output files and producing boxplots to illustrate the effects of these variations on the base model predictions. These plots are saved to a dedicated sensitivityPlots directory.

Model Calibration and Recalibration: To ensure the predicted risk probabilities are aligned with observed frequencies within a representative population, the system employs a recalibration process. Specifically, isotonic regression is used to recalibrate the metamodel predictions using a validation dataset from a large Scottish Health Board. The script evaluates the recalibrated model using k-fold cross validation to generate a brier score and generates calibration plots to visualise the model performance both before and after calibration.

Configuration and Execution: There is a configuration script that controls the configuration of the entire process. It defines the sensitivity coefficients, the names of the models to run, the features that will be adjusted, and generates a unique run ID to differentiate between different model runs. It also adjusts the time window values in the input files to create multiple prediction timeframes. A master script calls the configuration file, the model functions file, the sensitivity analysis script, and any additional scripts that are required. This script acts as the entry point for running the entire model development process.

In summary, the code provides a detailed framework for developing, validating and deploying clinical risk prediction models, and performs a recalibration within an independent external dataset to ensure consistent and accurate performance. The system is built using R, with python for the calibration and visualisation steps. The system is designed to be modular and maintainable, with clearly defined functions, and thorough documentation to ensure ongoing maintenance.

